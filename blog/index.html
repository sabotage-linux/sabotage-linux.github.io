<html><head><title>Devs on Acid</title>
<link rel="alternate" href="/blog/feed.rss" type="application/rss+xml" title="Devs on Acid RSS">
<link rel="shortcut icon" href="/favicon.ico" type="image/png">
<link rel="stylesheet" href="/blog/main.css" type="text/css">
</head>

<body>

<div class="header">
<h1>Devs on Acid</h1>
<ul class="menu">
<li><a class=amenu href="/blog/">Home</a>
<li><a class=amenu href="/blog/about">About</a>
<li><a class=amenu href="/blog/feed.rss">RSS</a>
</ul>
</div>

<div class="article"><h2><a href="4">benchmarking python bytecode vs interpreter speed and bazaar vs git</a></h2>

<div class="pub">07 Apr 2019 00:39 UTC</div>
<p>A couple weeks ago,
after an upgrade of libffi, we experienced odd build errors of python only on
systems where python had previously been installed with an older libffi version:</p>

<pre><code>error: [Errno 2] No such file or directory: '/lib/libffi-3.0.13/include/ffi.h'
</code></pre>

<p>There was no reference to <code>libffi-3.0.13</code> anywhere in the python source, and it
turned out that it was contained in old python .pyc/.pyo bytecode files that
survived a rebuild due to a packaging bug, and apparently were queried as
authorative during the python build.</p>

<pre><code>/lib/python2.7/_sysconfigdata.pyc:/lib/libffi-3.0.13/include
/lib/python2.7/_sysconfigdata.pyo:/lib/libffi-3.0.13/include
</code></pre>

<p>The packaging bug was that we didn't pre-generate .pyc/.pyo files just after the
build of python, so they would become part of the package directory in
/opt/python, but instead they were created on first access directly in
/lib/python2.7, resulting in the following layout:</p>

<pre><code>~ $ la /lib/python2.7/ | grep sysconfigdata
lrwxrwxrwx    1 root     root            48 Mar  4 03:11 _sysconfigdata.py -&gt; ../../opt/python/lib/python2.7/_sysconfigdata.py
-rw-r--r--    1 root     root         19250 Mar  4 03:20 _sysconfigdata.pyc
-rw-r--r--    1 root     root         19214 Jun 30  2018 _sysconfigdata.pyo
</code></pre>

<p>So on a rebuild of python, only the symlinks pointing to /opt/python were
removed, while the generated-on-first-use .pyc/.pyo files survived.</p>

<p>Annoyed by this occurence I started researching how generation of these bytecode
file could be suppressed, and it turned out that it can be controlled using a
<code>sys.dont_write_bytecode</code> variable, which in turn is set from the python C code.
Here's a <a href="https://gist.github.com/rofl0r/45d5c22a28ba346547ab2d0bad93bd7b">patch</a> doing that.</p>

<p>However, before turning off a feature that can potentially be a huge performance
boost, a responsible distro maintainer needs to do a proper benchmarking study
so he can make an educated decision.</p>

<p>So I developed a benchmark, that runs a couple of tasks using the <code>bazaar</code>
VCS system, which is written in python and uses a large amount of small files,
so the startup overhead should be significant. The task is executed 50 times, so
small differences in the host's CPU load due to other tasks should be evened
out.</p>

<p>The task is to generate a new bazaar repo, check 2 files and a directory into
bazaar in 3 commits, and print a log at the end.</p>

<script src="https://gist.github.com/rofl0r/0e48b188c0278f6f4dcc5e20b36b5105.js"></script>

<p>With bytecode generation disabled, the benchmark produced the following results:</p>

<pre><code>real    3m 15.75s
user    2m 15.40s
sys     0m 4.12s
</code></pre>

<p>With pregenerated bytecode, the following results were measured:</p>

<pre><code>real    1m 24.25s
user    0m 20.26s
sys     0m 2.55s
</code></pre>

<p>We can see, that in the case of a fairly big application like bazaar with
hundreds of python files, the precompilation does indeed make a quite noticable
difference. It is more than twice as fast.</p>

<p>What's also becoming apparent is that bazaar is slow as hell.
For the lulz, I replaced the bzr command in the above benchmark with git and
exported <code>PAGER=cat</code> so <code>git log</code> wouldn't interrupt the benchmark.
As expected, git is orders of magnitude faster:</p>

<pre><code>real    0m 0.48s
user    0m 0.02s
sys     0m 0.05s
</code></pre>

<p>Out of curiosity, I fiddled some more with python and added a <a href="https://gist.github.com/rofl0r/acd9da221c3c3f07213e87128ed37288">patch</a> that
builds python so its optimization switch <code>-O</code> is always active, and rebuilt both
python and bazaar to produce only .pyo files instead of .pyc.
Here are the results:</p>

<pre><code>real    1m 23.88s
user    0m 20.18s
sys     0m 2.54s
</code></pre>

<p>We can see that the optimization flag is next to useless. The difference is so
small it's almost not measurable.</p>

<p>Now this benchmark was tailored to measure startup compilation cost for a big
project, what about a mostly CPU-bound task unsing only a few python modules?</p>

<p>I modified a password bruteforcer to exit after a couple thousand rounds for
this purpose, and ran it 30x each without bytecode, with .pyc and .pyo each.</p>

<script src="https://gist.github.com/rofl0r/20ad5740cf741ce85d252b944e0b5ce9.js"></script>

<p>Here are the results:</p>

<p>No bytecode:</p>

<pre><code>real    3m 50.42s
user    3m 50.25s
sys     0m 0.03s
</code></pre>

<p>.pyc bytecode:</p>

<pre><code>real    3m 48.68s
user    3m 48.60s
sys     0m 0.01s
</code></pre>

<p>.pyo bytecode:</p>

<pre><code>real    3m 49.14s
user    3m 49.06s
sys     0m 0.01s
</code></pre>

<p>As expected, there's almost no difference between the 3. Funnily enough, the
optimized bytecode is even slower than the non-optimized bytecode in this case.</p>

<p>From my reading of this <a href="https://stackoverflow.com/questions/33781414/disable-python-basic-optimization-o">stackoverflow question</a>
it appears to me as if the .pyo bytecode differs from regular bytecode only in
that it lacks instructions for the omitted assert() calls, and eventually debug
facilities.</p>

<p>Which brings us back to the original problem: In order to have the .pyc files
contained in the package directory, they need to be generated manually during
the build, because apparently they're not installed as part of <code>make install</code>.
This can be achieved by calling</p>

<pre><code>./python -E Lib/compileall.py "$dest"/lib/python2.7
</code></pre>

<p>after make install finished.
With that achieved, i compared the size of the previous /opt/python directory
without .pyc files with the new one.</p>

<p>It's 22.2 MB vs 31.1MB, so the .pyc files add roughly 9MB and make the package
almost 50% bigger.</p>

<p>Now it happens that some python packages, build scripts and the like call python
with the optimization flag <code>-O</code>. this causes our previous problem to re-appear,
now we will have stray .pyo files in /lib/python2.7.</p>

<p>So we need to pregenerate not only .pyc, but also .pyo for all python modules.
This will add another 9MB to the python package directory.</p>

<p>OR... we could simply <strong>turn off</strong> the ability to activate the optimised mode,
which as we saw, is 99.99% useless.
This seems to be the most reasonable thing to do, and therefore this is
precisely what I <a href="https://github.com/sabotage-linux/sabotage/commit/145a77a0ced2a628b52d700071fae7e87f830010">now implemented in sabotage linux</a>.</p>
<p><a href="4/#comments">Post or read comments...</a></p></div>
<div class="article"><h2><a href="3">the rusty browser trap</a></h2>

<div class="pub">06 Apr 2019 11:55 UTC</div>
<p>If you're following <a href="https://github.com/sabotage-linux/sabotage/">sabotage linux</a>
development, you may have noticed that we're stuck on Firefox 52esr, which was
released over a year ago.
This is because non-optional parts of Firefox were rewritten in the "Rust"
programming language, and all newer versions now require to have a Rust compiler
installed.</p>

<h4>And that is a real problem.</h4>

<p>The Rust compiler is written in Rust itself, exposing the typical hen-and-egg
problem.
Its developers have used previous releases in binary form along the path of
evolution of the language and its compiler. This means in practice that one can
only build a rust compiler by using a binary build supplied by a third party,
which in turn basically means that one has to trust this third party.
Assuming that the binary actually works on one's own system.</p>

<p>As sabotage linux is based on <a href="http://www.musl-libc.org/">musl</a>, the latter is
not self-evident.</p>

<p>Traditionally, the only binary thing required to bootstrap sabotage linux was
a C compiler. It was used to build the stage0 C compiler, which was then used
to build the entire system. A sabotage user can have high confidence that his
OS does not contain any backdoors in the userland stack.
Of course, it's impossible to read all the millions of lines of code of the
linux kernel, nor is it possible to know the backdoors inside the CPU silicon or
in the software stack that runs on the BIOS level or below.
Still, it is a pretty good feeling to have at least a trustworthy userland.</p>

<p>So Rust developers want you to slap a binary containing megabytes of machine
instructions on your PC and execute it.</p>

<p>If we assume for one moment that we are OK with that, the next problem is that
we now need a different binary for every architecture we support.
There's no mechanism in sabotage that allows to download a different thing
per-architecture. All existing packages are recipes on how to build a piece of
software from source, and that's done with the identical sources for all
platforms.</p>

<p>Additionally, <a href="https://forge.rust-lang.org/platform-support.html">Rust doesn't actually support all architectures</a> we support.
It's a hipster thing, and not a professional product. And the hipsters decided
to support only a very small number of popular architectures, such as AMD64 and
x86. Others are either not supported at all, or without guarantee that it'll
work.</p>

<p>So even if we embrace Rust, there will be some architectures that can't have a
working Firefox - ever?</p>

<p>Now somebody who probably likes Rust, decided he wants to write a <a href="https://github.com/thepowersgang/mrustc">compiler</a> for
it in C++, so people can use it to bootstrap from source. However, he targets
a pretty old version of it, so in order to get a version compiled that's recent
enough to build Firefox's sources, one needs to
<a href="https://www.gnu.org/software/guix/blog/2018/bootstrapping-rust/">build a chain of 12+ Rust versions</a>.
A member of our team actually embarked on this voyage, but the result was pretty
disillusioning.</p>

<ul>
<li>The build of mrustc (the Rust compiler written in C++) itself takes over an
hour on a very fast 8core workstation, and requires several patches.</li>
<li>Every Rust version that needs to be built along the way bundles it own version
of LLVM. LLVM is a huge library full of bugs, and never had proper musl support
upstreamed. There are dozens of different build errors and hard-to-debug
crashes for <em>every single different LLVM version</em> involved.
Each bootstrapping step here takes a minimum of an hour too, as compiling
Rust is even slower than compiling C++.</li>
</ul>

<p>After our team member spent about 3 nights on this endeavour, he gave up, even
though we had support from somebody of "<a href="https://www.adelielinux.org/">adelie linux</a>", who went throught the
entire process already. unfortunately, that person didn't take any
step-by-step notes, there's only a
<a href="https://portage.smaeul.xyz/distfiles/bootstrap">repository of mostly unsorted patches and other files</a>
and a <a href="https://github.com/smaeul/rust/commits/bootstrap-1.19.0">patched version of rust 1.19.0</a>
to start with.
(<a href="https://adelie.blog/2017/11/18/our-official-stance-on-the-rust-programming-language/">Here</a>'s a blog post from adelie linux authors about rust, btw).</p>

<p>So could it be done? Most likely yes, but it would require me to spend about 2
estimated weeks of work, digging in the C++ turd of LLVM and Rust.
Certainly not anything I would like to spend my time on.
Unlike the people from adelie linux, my goal is not to create a single set of
bootstrap binaries to be used in the future, but package recipes, so a user can
build the entire set of rust versions from source.
Building them all will probably require almost two full days of CPU time on a
very fast box, so this is something not everybody can even afford to do.</p>

<p>So from my point of view, it looks pretty much as if Firefox is dead.
By choosing to make it exclusive to owners of a Rust compiler, mozilla chose
to make it hard-to-impossible for hobbyists and source code enthusiasts like
myself to compile their browser themselves.</p>

<p>Not that it was easy in the past either, every version bump required about a
half day of effort to fix new issues, introduced in this giant pile of C++
copy-pasted from dozens of differents projects, and held together by a fragile
build system mix of python, shell, perl, ancient autoconf etc etc...</p>

<p>None of those upstream sources were ever tested on musl-based linux systems by
their developers, and sabotage's unconventional filesystem layout adds yet
another layer of possible breakage especially regarding the python virtualenv
based build system.</p>

<h4>So, Firefox is dead. What's the alternative?</h4>

<p>Chromium? Possibly, but it's a clusterfuck itself.
The <a href="http://www.linuxfromscratch.org/blfs/view/8.2/xsoft/chromium.html">source tarball is about 0.5 GB</a>
<strong>compressed</strong>. and requires 2+GB hdd space just to unpack the sources, and
probably another 5 GB for temporary object files during the build.
And it will takes hours and hours to build, if you even have enough RAM.
That's not really compatible with a hobbyist project, besides the numerous
privacy issues with this browser.</p>

<p>The only viable option left might be a webkit based browser or
<a href="http://www.palemoon.org/">palemoon</a>, a fork of firefox without rust.</p>

<p>I even considered for a while to run a QEMU VM with ReactOS with a binary
windows-based precompiled browser, but funnily enough, around the same
time mozilla started giving the boot to open-source enthusiasts by requiring
Rust, they also removed support for Windows XP.
And subsequently for ReactOS, since it is based on the Win2K3 API.</p>

<p>So the future looks pretty grim.
We need to invest a lot of work trying to get Palemoon to compile, and hopefully
it will stay rust-free and usable for a couple more years.
If not, we will be forced to run a VM with a bloated GLIBC-based linux distro
and the full X11 stack, just to run a browser.</p>

<p>Because unfortunately, without an up-to-date browser, a desktop system is almost
worthless.</p>
<p><a href="3/#comments">Post or read comments...</a></p></div>
<div class="article"><h2><a href="2">how compatible is libreSSL ?</a></h2>

<div class="pub">12 Jul 2014</div>
<h4>portability</h4>

<p>yesterday the "portable" version of libressl was released.
http://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.0.0.tar.gz</p>

<p>i set up a package in <a href="https://github.com/sabotage-linux/sabotage">sabotage linux</a>,
and went on a voyage to investigate whether the full set of packages can be
used with libressl instead of openssl.</p>

<p>first of all, i had to fight some obstacles to get libressl compiling though...</p>

<h5>obstacle 1 - -Werror</h5>

<pre><code>../include/openssl/bio.h:622:3: error: '__bounded__' attribute directive ignored [-Werror=attributes]
</code></pre>

<p>-Werror is hardcoded in the configure script, which is a very bad idea, and the opposite of portable.
using -Werror is a guarantueed build break whenever the build is tried on a system the original developer had no access to.
it's sufficient to use a different compiler version, different libc version, etc to make new warnings pop up.</p>

<p>fixed with</p>

<pre><code>sed -i 's/-Werror//' configure
</code></pre>

<h5>obstacle 2 - unconditional inclusion of internal glibc header</h5>

<pre><code>compat/issetugid_linux.c:7:30: fatal error: gnu/libc-version.h: No such file or directory
</code></pre>

<p>many people assume linux == glibc, but that is not the reality.
sabotage linux uses <a href="http://www.musl-libc.org/">musl libc</a>, and there are at least 4 other libcs that could be used instead (uclibc, dietlibc, klibc, bionic).</p>

<p>looking at <a href="https://gist.github.com/rofl0r/7f2901ca1249e7b55d62">issetugid<em>linux.c</a> uncovers a dubious hack:
if glibc 2.19 is detected, getauxval(AT</em>SECURE) is not used, because there was once a bug (see comment in source code).</p>

<p>however it's common practice in distros to backport bugfixes, without updating the version number.
so this hack prevents proper usage of getauxval even if your libc version is long fixed.
the mentioned bug is very likely already fixed in any distro using glibc 2.19.</p>

<p>to get the thing out of my way and compilation going on, the quick fix was to cover everything with <code>#ifdef __GLIBC__</code>.
what the code really should do though is to just use the getauxval call unconditionally without the glibc version check.</p>

<h5>obstacle 3 - unnecessary unconditional inclusion of sys/sysctl.h</h5>

<pre><code>compat/getentropy_linux.c:27:24: fatal error: sys/sysctl.h: No such file or directory
</code></pre>

<p>musl does not have sys/sysctl.h, because:
(citing musl's author Rich Felker)</p>

<blockquote>
  <p>sysctl does not work, and NEVER worked. using it is bogus.
it was a bogus experimental syscall that was deprecated before it was ever used (basically, a broken binary version of /proc/sys, without any stability between kernel versions for what the binary constants meant).</p>
</blockquote>

<p>since the code in question does not use the sysctl function (declared in sys/sysctl.h) and does the syscall() directly,
it was safe and sufficient to just remove the include statement.</p>

<p>still it leaves a bad taste in my mouth that it was used at all...</p>

<p>having fixed these 3 issues, libressl built successfully.
<a href="https://github.com/sabotage-linux/sabotage/commit/4f2da253f669a0e2e69e6d5607e56c552b716eff">commit 4f2da253</a></p>

<p>on the plus side: using 8 cores, libressl builds in about 1 minute, while openssl requires 1:45.
also openssl depends on perl, which takes an additional 2 minutes buildtime.
so if nothing else depends on perl, it's about 3x faster.</p>

<h4>compatibility</h4>

<p>with libressl in place, a "world" metapackage (contains almost all packages) build was started.
the results:</p>

<p>wget failed to build due to lack of RAND_egd() function.
fixed by using a patch from openbsd.
<a href="https://github.com/sabotage-linux/sabotage/commit/234185c0dd408a9eeb6a14a31719760841f97be5">commit 234185c0</a></p>

<p>stunnel failed to build due to lack of RAND_egd() function.
fixed by using a custom patch conceptually equivalent to the wget one.
<a href="https://github.com/sabotage-linux/sabotage/commit/9b47cbbf3ce903dee042c45c8197db066e8e0053">commit 9b47cbb</a></p>

<p>cryptsetup and others failed to detect openssl due to lack of pkgconfig files.
i modified my package build script to create these .pc files (copies from openssl).
<a href="https://github.com/sabotage-linux/sabotage/commit/156a36253b9e19f83103135113451209796c39cf">commit 156a362</a></p>

<p>php, xorg-server and others failed to build subtly due to an ugly hack used in libressl's libcompat.a, linked into libcrypto.so:</p>

<pre><code>$ gcc test.c -lcrypto -fvisibility=hidden
/bin/ld: a.out: hidden symbol `main' in /tmp/ccobhDjc.o is referenced by DSO
/bin/ld: final link failed: Bad value

$ readelf -a /lib/libcrypto.so | grep main
000000345708 000a00000006 R_X86_64_GLOB_DAT 0000000000000000 main + 0
10: 0000000000000000 0 NOTYPE GLOBAL DEFAULT UND main
2146: 0000000000000000 0 NOTYPE GLOBAL DEFAULT UND main
</code></pre>

<p>in getentropy_linux.c:</p>

<pre><code>extern int main(int, char *argv[]);
#define HD(x) (SHA512_Update(&amp;ctx, (char *)&amp;(x), sizeof (x)))
HD(main); /* an addr in program */
</code></pre>

<p>the address of main() is used to gather entropy… very smart… NOT.</p>

<p>most of the methods used in this file to gather entropy are very dubious.
the crypto experts from OpenBSD should know better and just use /dev/urandom and/or getauxval(AT_RANDOM)
instead of all these hacks.</p>

<p><a href="https://github.com/sabotage-linux/sabotage/commit/1a8113699311c95a42c0cce5d09746ca290852ac">commit 1a81136</a></p>

<p>with that fixed, most programs expecting openssl seem to compile and work correctly.</p>

<p>mysql's CMAKE build system fails to detect SSL libraries.</p>

<pre><code>-- OPENSSL_INCLUDE_DIR = /usr/include
-- OPENSSL_LIBRARY = /usr/lib/libssl.so
-- CRYPTO_LIBRARY = /usr/lib/libcrypto.so
-- OPENSSL_MAJOR_VERSION = 2
-- Looking for SHA512_DIGEST_LENGTH
-- Looking for SHA512_DIGEST_LENGTH - found
CMake Error at cmake/ssl.cmake:231 (MESSAGE):
Cannot find appropriate system libraries for SSL. Use WITH_SSL=bundled to
enable SSL support
</code></pre>

<p>not patched yet.</p>

<p>the last build error was in apache:</p>

<pre><code>ssl_engine_init.c:445:28: error: `ENGINE_CTRL_CHIL_SET_FORKCHECK' undeclared
</code></pre>

<p>this is a macro which is available in openssl's engine.h, and was removed from libressl for unknown reasons.
not patched yet.</p>

<p>apart from these two, everything seems to be usable without big effort.
so if the libressl developers rip out all their dubious entropy generation methods in favor of /dev/urandom on linux it might be well worth switching to it.</p>

<p>the whole adventure is documented in the <a href="https://github.com/sabotage-linux/sabotage/commits/libressl_replaces_openssl">libressl<em>replaces</em>openssl branch</a>.</p>

<h4>Update 07/13</h4>

<p>OpenBSD released an updated version 2.0.1 earlier today.
the new release fixes the following problems
- reference to main() which breaks packages using -fvisibility=hidden
- usage of -Werror
- generation of pkg-config files
- unconditional inclusion of sys/sysctl.h</p>

<p>so the portability concerns have largely been addressed.
the only portability issue not fixed is the glibc-specific stuff in issetugid_linux.c.
instead, <a href="http://openwall.com/lists/musl/2014/07/12/28">a patch containing an issetugid implementation for inclusion in musl
was sent to the musl maillist</a>.</p>

<p>on the application compatibility side nothing seems to have changed.
RAND_egd() is still missing, as well as the macros used by apache.</p>

<p>the dubious fallbacks for getentropy (obsolete sysctl syscall, function addresses) are still present.</p>

<p><a href="https://blog.hboeck.de/archives/851-LibreSSL-on-Gentoo.html">this blog</a> about similar testing done on gentoo (recommended read) has a link to a patch for the apache build.
<a href="https://svn.hboeck.de/libressl-overlay/net-misc/openssh/files/openssh-arc4random.diff">there is also a patch for a segfault in openssh</a>.</p>

<h5>this blog post originally appeared on my currently defunct <a href="https://devsonacid.wordpress.com/2014/07/12/how-compatible-is-libressl/">wordpress blog</a></h5>
<p><a href="2/#comments">Post or read comments...</a></p></div>
<div class="article"><h2><a href="1">sqlite's anal gamation</a></h2>

<div class="pub">25 Sep 2013</div>
<p>sqlite's slogan: <strong>"Small. Fast. Reliable. Choose any three."</strong></p>

<p>i always wondered though, how such a small or "lite" package can take such a
considerable amount of time to build.</p>

<p>as the main author of the sabotage linux distribution, building software is my
daily bread, so i own a pretty fast build box.
it's an 8 core machine with 3.1 GHz, which builds a complete 3.11 linux kernel
in less than 5 minutes, making use of all 8 cores via the nice parallel build
feature of GNU make.</p>

<pre><code>make -j8
</code></pre>

<p>when invoking make like this, it first determines the dependencies between the
translation units,
and then runs up to 8 build processes, one per cpu core, each one building a
different .c file.</p>

<p>GCC 3.4.6, a C compiler with full C99 support builds in 43 sec:</p>

<pre><code>$ time butch rebuild gcc3
2013.09.25 12:13:50 building gcc3 (/src/build/build_gcc3.sh) -&gt; /src/logs/build_gcc3.log
2013.09.25 12:14:33 done.
real 0m 43.97s
user 1m 36.66s
sys 0m 13.74s
</code></pre>

<p>however, for sqlite, a supposedly small package, build times are comparatively huge:</p>

<pre><code>$ time butch rebuild sqlite
2013.09.25 12:18:27 building sqlite (/src/build/build_sqlite.sh) -&gt; /src/logs/build_sqlite.log
2013.09.25 12:19:21 done.
real 0m 54.03s
user 0m 52.02s
sys 0m 1.51s
</code></pre>

<p>nearly one minute, a fifth of the time used to build the linux kernel and 10
seconds more than the gcc compiler.</p>

<p>the full-blown postgresql database server package, takes <strong>less</strong> time to build
as well:</p>

<pre><code>$ time butch rebuild postgresql
2013.09.25 12:19:21 building postgresql (/src/build/build_postgresql.sh) -&gt; /src/logs/build_postgresql.log
2013.09.25 12:19:57 done.
real 0m 36.63s
user 1m 53.34s
sys 0m 12.03s
</code></pre>

<p>how is it possible that postgresql, shipping 16 MB of compressed sources, as
opposed to 1.8MB of sqlite, builds 33% faster ?</p>

<p>if you look at the user times above, you start getting an idea.
the user time (i.e. the entire cpu time burnt in userspace) for postgresql is
1m53, while the total time that actually passed was only 36s.</p>

<p>that means that the total work of 113 seconds was distributed among multiple
cpu cores.
dividing the user time through the real time gives us a concurrency factor of
3.13.
not perfect, given that make was invoked with -j8, but much better than sqlite,
 which apparently only used a single core.</p>

<p>let's take a look at sqlite's builddir</p>

<pre><code>$ find . -name '*.c'
./sqlite3.c
./shell.c
./tea/generic/tclsqlite3.c
./tea/win/nmakehlp.c
</code></pre>

<p>ah, funny. there are only 4 C files total. that partially explains why 8 cores
didn't help.
the 2 files in tea/ are not even used, which leaves us with</p>

<pre><code>$ ls -la *.c
-rw-r--r-- 1 root root 91925 Jan 16 2012 shell.c
-rw-r--r-- 1 root root 4711082 Jan 16 2012 sqlite3.c
</code></pre>

<p>so in the top level builddir, there are just 2 C files, one being 90 KB, and
the other roughly 5MB.
the 90KB version is built in less than 1 second, so after that the entire time
spent is waiting for the single cpu core building the huge sqlite3.c.</p>

<p>so why on earth would somebody stuff all source code into a single translation
unit and thereby defeat
makefile parallellism ?</p>

<p>after all, the IT industry's mantra of the last 10 years was
"parallellism, parallellism, and even more parallellism".</p>

<p>here's the explanation:
https://www.sqlite.org/amalgamation.html</p>

<p>it's a "feature", which they call amalgamation.</p>

<p>i call it anal gamation.</p>

<blockquote>
  <p>In addition to making SQLite easier to incorporate into other projects, the amalgamation also makes it run faster.
Many compilers are able to do additional optimizations on code when it is contained with in a single translation unit such as it is in the amalgamation. </p>
</blockquote>

<p>so they have 2 reasons for wasting our time:</p>

<ul>
<li>reason 1: easier to incorporate</li>
<li>reason 2: generated code is better as the compiler sees all code at once.</li>
</ul>

<p>let's look at <strong>reason 1</strong>:
what they mean with incorporation is embedding the sqlite source code into another projects source tree.</p>

<p>it is usually considered bad practice to embed third-party source code into your own source tree, for multiple reasons:</p>

<ul>
<li>every program that uses its own embedded copy of library X does not benefit from security updates when the default library install is updated.</li>
<li>we have multiple different versions on the harddrive and loaded in RAM, wasting system resources</li>
<li>having multiple incompatible versions can lead to a lot of breakage when it's used from another lib:
for example application X uses lib Y and lib Z, and lib Z uses a "incorporated" version of lib Y.
so we have a nice clash of 2 different lib Y versions. if lib Y has global state, it will get even worse.</li>
<li>if the library in question is using some unportable constructs, wrong ifdefs etc., it needs to be patched to build.
having to apply and maintain different sets of patches against multiple different versions "incorporated" into other packages, represents a big burden for the packager.</li>
</ul>

<p>instead, the installed version of libraries should be used.</p>

<p>pkg-config can be used to query existence, as well as CFLAGS and LDFLAGS needed
to build against the installed version of the library. if the required library
is not installed or too old, just throw an error at configure time and tell
the user to install it via apt-get or whatever.</p>

<h3>conclusion: "incorporation" of source code is a bad idea to begin with.</h3>

<p>now let's look at <strong>reason 2</strong> (better optimized code):
it possibly sometimes made sense to help the compiler do its job in the 70ies, when everything started.
however, it's 2013 now.
compilers do a great job optimizing, and they get better at it every day.</p>

<p>since GCC 4.5 was released in 2010, it ships with a feature called LTO
it builds object files together with metadata that allows it to strip off unneeded functions and variables, inline functions that are only called once or twice, etc at link time - pretty much anything the sqlite devs want to achieve, and probably even more than that.</p>

<h3>conclusion: pseudo-optimizing C code by stuffing everything into a big file is obsolete since LTO is widely available.</h3>

<p>LTO does a better job anyway - not that it matters much, as sqlite spends most time waiting for I/O.
every user who wants to make sqlite run faster, can simply add -flto to his CFLAGS.
there's no need to dictate him which optimization he wants to apply.
following this logic, they could as well just ship generated assembly code…</p>

<p>but hey - we have the choice !
<a href="https://www.sqlite.org/download.html">here</a>'s actually a tarball containing
the ORIGINAL, UN-ANAL-GAMATED SOURCE CODE...</p>

<p>... just that it's not a tarball.</p>

<p>it's a fscking <strong>ZIP</strong> file.
yes, you heard right.
they distribute their source as ZIP files, treating UNIX users as second-class
citizens.</p>

<p>additionally they say that you should not use it:</p>

<blockquote>
  <p>sqlite-src-3080002.zip (5.12 MiB)
A ZIP archive of the complete source tree for SQLite version 3.8.0.2 as extracted from the version control system.
The Makefile and configure script in this tarball are <strong>not supported</strong>.
<strong>Their use is not recommended</strong>.
The SQLite developers do not use them.
You <strong>should not use</strong> them either.
If you want a configure script and an automated build, use either the amalgamation tarball or TEA tarball instead of this one.
To build from this tarball, hand-edit one of the template Makefiles in the root directory of the tarball and build using your own customized Makefile.</p>
</blockquote>

<p>Note how the text talks about "this tarball" despite it being a ZIP file.</p>

<p>Fun. there's only a single TARball on the entire site, so that's what you naturally pick for your build system.
and that one contains the ANAL version.
Note that my distro's build system does not even support zip files, as i don't have a single package in my repo that's not building from a tarball.
should i change it and write special case code for one single package which doesn't play by the rules ?
i really don't think so.</p>

<p>funny fact: they even distribute <strong>LINUX BINARY downloads</strong> as .zip.
i wonder in which world they live in.</p>

<p>why do i care so much about build time ? it's just a minute after all.
because the distribution gets built over and over again. and it's not just me building it, but a lot of other people as well - so the cumulated time spent waiting for sqlite to finish building its 5 MB file gets bigger and bigger each day.
in the past i built sqlite more than 200 times, so my personal cumulated wasted time on it already exceeds the amount of time i needed to write this blog post.</p>

<p>so what i hope to see is sqlite</p>

<ul>
<li>using tarballs for all their sources (and eventually distribute an additional .zip for windows lusers)</li>
<li>using tarballs for their precompiled linux downloads</li>
<li>either getting rid of the anal version entirely, now that they learned about LTO, or offer the anal version as an additional download and do not discourage users from using the sane version.</li>
</ul>

<h3>Update:</h3>

<p>I just upgraded sqlite from 3071000 to 3080002</p>

<pre><code>2013.09.27 02:23:24 building sqlite (/src/build/build_sqlite.sh) -&gt; /src/logs/build_sqlite.log
2013.09.27 02:25:31 done.
</code></pre>

<p>it now takes more than 2 minutes.</p>

<h5>this blog post originally appeared on my currently defunct <a href="https://devsonacid.wordpress.com/2013/09/25/sqlites-anal-gamation/">wordpress blog</a></h5>
<p><a href="1/#comments">Post or read comments...</a></p></div>



</body>
</html>
